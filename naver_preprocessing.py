"""
ì›¹íˆ° ì»· ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ë¡œì»¬ í™˜ê²½ - ì™„ì „íŒ)
- ë°°ê²½ìƒ‰ ë³´ì¡´ ì ì‘í˜• ë¦¬ì‚¬ì´ì¦ˆ
- ê³¼ë„í•œ ì—¬ë°± í•„í„°ë§
- pHash ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì„ íƒ)
- í•œê¸€ ê²½ë¡œ ì™„ë²½ ì§€ì›
- ë¡œì»¬ ë””ë ‰í† ë¦¬ ì €ì¥

í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
pip install pillow imagehash opencv-python tqdm numpy
"""

# ==================== 1. í™˜ê²½ ì„¤ì • ====================
import cv2
import numpy as np
from PIL import Image
import imagehash
from pathlib import Path
import shutil
from tqdm import tqdm
import json
import zipfile
import hashlib


# ==================== 2. ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ====================

def estimate_background_color(image, border_width=5):
    """ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ í”½ì…€ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ë°°ê²½ìƒ‰ ì¶”ì •"""
    h, w = image.shape[:2]
    edges = np.concatenate([
        image[:border_width, :].reshape(-1, 3),
        image[-border_width:, :].reshape(-1, 3),
        image[:, :border_width].reshape(-1, 3),
        image[:, -border_width:].reshape(-1, 3)
    ])
    return np.median(edges, axis=0).astype(np.uint8)


def filter_excessive_spacing(image, max_spacing_ratio=0.7):
    """ê³¼ë„í•œ ì—¬ë°± ì»· í•„í„°ë§ (í¬ë ˆë”§/ê´‘ê³  ì œê±°)"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # ë°°ê²½ìƒ‰ ì˜ì—­ ë¹„ìœ¨ ê³„ì‚°
    bg_color = estimate_background_color(image)
    bg_gray = np.mean(cv2.cvtColor(bg_color.reshape(1, 1, 3), cv2.COLOR_BGR2GRAY))
    bg_mask = np.abs(gray - bg_gray) < 20
    spacing_ratio = np.mean(bg_mask)
    
    # ì—¬ë°±ì´ 70% ì´ìƒì´ë©´ ì œì™¸
    if spacing_ratio > max_spacing_ratio:
        return None, spacing_ratio
    
    return image, spacing_ratio


def smart_crop_vertical_spacing(image, min_content_height=50):
    """ìƒí•˜ ê³¼ë„í•œ ì—¬ë°±ë§Œ ì œê±° (ì¢Œìš°ëŠ” ìœ ì§€)"""
    h, w = image.shape[:2]
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # í–‰ë³„ ì½˜í…ì¸  ë°€ë„ ê³„ì‚°
    row_content = np.mean(gray, axis=1)
    bg_value = np.median([row_content[0], row_content[-1]])
    content_rows = np.abs(row_content - bg_value) > 10
    
    # ì½˜í…ì¸  ìˆëŠ” í–‰ ë²”ìœ„ ì°¾ê¸°
    content_indices = np.where(content_rows)[0]
    if len(content_indices) == 0:
        return image
    
    top = max(0, content_indices[0] - 5)  # ì—¬ìœ  5px
    bottom = min(h, content_indices[-1] + 5)
    
    # ìµœì†Œ ë†’ì´ ë³´ì¥
    if bottom - top < min_content_height:
        return image
    
    return image[top:bottom, :]


def adaptive_resize_with_original_bg(image, target_size=320):
    """
    ëª¨ë“  ì´ë¯¸ì§€ë¥¼ TARGET_SIZE x TARGET_SIZE ì •ë°©í˜•ìœ¼ë¡œ í†µì¼. (ë¹„ìœ¨ ë³´ì¡´)
    """
    h, w = image.shape[:2]
    
    # 1. ğŸ¯ ê¸´ ë³€ì„ target_sizeì— ë§ì¶”ì–´ ë¹„ìœ¨ ìœ ì§€
    if h > w:
        scale = target_size / h 
    else:
        scale = target_size / w 
        
    new_h, new_w = int(h * scale), int(w * scale)
    
    # ë¦¬ì‚¬ì´ì¦ˆ ì‹œ 0 í¬ê¸° ë°©ì§€ ë° ë¦¬ì‚¬ì´ì¦ˆ
    if new_h == 0 or new_w == 0:
        new_h = max(1, new_h); new_w = max(1, new_w) # ìµœì†Œ 1px ë³´ì¥
        
    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
    
    # 2. ë°°ê²½ìƒ‰ ìë™ ì¶”ì •
    bg_color = estimate_background_color(image)
    bg_color = tuple(map(int, bg_color)) 
    
    # 3. TARGET_SIZE x TARGET_SIZE ì •ì‚¬ê°í˜•ìœ¼ë¡œ íŒ¨ë”©
    # íŒ¨ë”© ëª©í‘œ í¬ê¸°ëŠ” target_size
    pad_top = (target_size - new_h) // 2
    pad_bottom = target_size - new_h - pad_top
    pad_left = (target_size - new_w) // 2
    pad_right = target_size - new_w - pad_left
    
    padded = cv2.copyMakeBorder(
        resized, 
        pad_top, pad_bottom, pad_left, pad_right, 
        cv2.BORDER_CONSTANT, 
        value=bg_color
    )
    
    # ìµœì¢… ê²°ê³¼ëŠ” í•­ìƒ target_size x target_size
    return padded


def perceptual_hash_with_masking(image):
    """ë°°ê²½ ë¬´ì‹œí•œ pHash ê³„ì‚° (ì—ì§€ ê¸°ë°˜)"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    return imagehash.phash(Image.fromarray(edges))


def extract_style_features(image):
    """ìŠ¤íƒ€ì¼ íŠ¹ì„± ì¶”ì¶œ (ë©”íƒ€ ì„ë² ë”©ìš©)"""
    # 1. ë°°ê²½ìƒ‰ ë¶„í¬
    bg_color = estimate_background_color(image)
    bg_hsv = cv2.cvtColor(bg_color.reshape(1, 1, 3), cv2.COLOR_BGR2HSV)[0, 0]
    
    # 2. ì—¬ë°± ë¹„ìœ¨
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    bg_gray = np.mean(cv2.cvtColor(bg_color.reshape(1, 1, 3), cv2.COLOR_BGR2GRAY))
    spacing_ratio = np.mean(np.abs(gray - bg_gray) < 20)
    
    # 3. ìƒ‰ìƒ ì±„ë„ í‰ê· 
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    saturation_mean = np.mean(hsv[:, :, 1])
    
    # 4. ë°ê¸° í‰ê· 
    brightness_mean = np.mean(hsv[:, :, 2])
    
    return {
        'bg_hue': float(bg_hsv[0]),
        'bg_saturation': float(bg_hsv[1]),
        'bg_brightness': float(bg_hsv[2]),
        'spacing_ratio': float(spacing_ratio),
        'content_saturation': float(saturation_mean),
        'content_brightness': float(brightness_mean)
    }


# ==================== 3. ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ====================

def preprocess_webtoon_cut(image_path, target_size=320):
    """ì›¹íˆ° ì»· ì „ì²˜ë¦¬ ì¢…í•© íŒŒì´í”„ë¼ì¸ (í•œê¸€ ê²½ë¡œ ëŒ€ì‘)"""
    try:
        # í•œê¸€ ê²½ë¡œ ì²˜ë¦¬: numpyë¡œ ì½ê¸°
        img_array = np.fromfile(str(image_path), np.uint8)
        image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
        
        if image is None:
            return None, None, "Failed to read image"
        
        # 1ë‹¨ê³„: ê³¼ë„í•œ ì—¬ë°±/í¬ë ˆë”§ ì»· í•„í„°ë§
        filtered, spacing_ratio = filter_excessive_spacing(image)
        if filtered is None:
            return None, None, f"Excessive spacing: {spacing_ratio:.2f}"
        
        # 2ë‹¨ê³„: ìƒí•˜ ê·¹ë‹¨ ì—¬ë°± ì œê±°
        cropped = smart_crop_vertical_spacing(filtered)
        
        # 3ë‹¨ê³„: ì ì‘í˜• ë¦¬ì‚¬ì´ì¦ˆ (ë°°ê²½ìƒ‰ ë³´ì¡´)
        resized = adaptive_resize_with_original_bg(cropped, target_size=target_size)
        
        # 4ë‹¨ê³„: ìŠ¤íƒ€ì¼ íŠ¹ì„± ì¶”ì¶œ
        style_features = extract_style_features(resized)
        
        return resized, style_features, "Success"
    
    except Exception as e:
        return None, None, f"Error: {str(e)}"


def deduplicate_cuts(image_list, hash_threshold=2):
    """pHash ê¸°ë°˜ ê·¼ì ‘ì¤‘ë³µ ì œê±° (í•œê¸€ ê²½ë¡œ ëŒ€ì‘)"""
    hashes = {}
    unique_images = []
    duplicates = []
    
    print("\nğŸ” ì¤‘ë³µ ì œê±° ì¤‘...")
    for img_path in tqdm(image_list, desc="Deduplicating"):
        try:
            # í•œê¸€ ê²½ë¡œ ì²˜ë¦¬: numpyë¡œ ì½ê¸°
            img_array = np.fromfile(str(img_path), np.uint8)
            img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
            
            if img is None:
                print(f"âš ï¸ ì½ê¸° ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {img_path.name}")
                continue
            
            h = perceptual_hash_with_masking(img)
            
            # ê¸°ì¡´ í•´ì‹œì™€ ë¹„êµ
            is_duplicate = False
            for existing_hash, existing_path in hashes.items():
                if h - existing_hash <= hash_threshold:
                    is_duplicate = True
                    duplicates.append((img_path, existing_path))
                    break
            
            if not is_duplicate:
                hashes[h] = img_path
                unique_images.append(img_path)
        
        except Exception as e:
            # ì¡°ìš©íˆ ê±´ë„ˆëœ€ (ë„ˆë¬´ ë§ì€ ê²½ê³  ë°©ì§€)
            continue
    
    return unique_images, duplicates


def remove_exact_duplicates(image_list):
    """íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ ì œê±° (ë¹ ë¥´ê³  ì •í™•)"""
    print("\nğŸ” ì™„ì „ ì¤‘ë³µ íŒŒì¼ ì œê±° ì¤‘...")
    seen_hashes = {}
    unique_images = []
    duplicates = []
    
    for img_path in tqdm(image_list, desc="Checking exact duplicates"):
        try:
            # íŒŒì¼ ë‚´ìš© í•´ì‹œ ê³„ì‚°
            with open(img_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            
            if file_hash in seen_hashes:
                duplicates.append((img_path, seen_hashes[file_hash]))
            else:
                seen_hashes[file_hash] = img_path
                unique_images.append(img_path)
        except Exception as e:
            # ì½ê¸° ì‹¤íŒ¨í•œ íŒŒì¼ì€ ê±´ë„ˆëœ€
            continue
    
    print(f"âœ… ì™„ì „ ì¤‘ë³µ ì œê±°: {len(duplicates)}ê°œ ì œê±°, {len(unique_images)}ê°œ ìœ ì§€")
    return unique_images, duplicates


# ==================== 4. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ ====================

def process_webtoon_directory(
    input_dir, 
    output_dir, 
    target_size=320, 
    remove_duplicates=False,
    hash_threshold=2,
    remove_exact_duplicates_only=True
):
    """
    ì›¹íˆ° ë””ë ‰í† ë¦¬ ì „ì²´ ì „ì²˜ë¦¬
    
    Parameters:
    - input_dir: ì›ë³¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    - output_dir: ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    - target_size: ë¦¬ì‚¬ì´ì¦ˆ ëª©í‘œ í¬ê¸° (ì§§ì€ ë³€ ê¸°ì¤€)
    - remove_duplicates: pHash ê¸°ë°˜ ìœ ì‚¬ ì¤‘ë³µ ì œê±° ì—¬ë¶€
    - hash_threshold: pHash í•´ë° ê±°ë¦¬ ì„ê³„ê°’
    - remove_exact_duplicates_only: ì™„ì „ ì¤‘ë³µë§Œ ì œê±° (ê¶Œì¥)
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    
    # ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not input_path.exists():
        print(f"âŒ ì…ë ¥ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")
        return 0, 0, 0
    
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
    image_extensions = {'.jpg', '.jpeg', '.png', '.webp'}
    image_files = []
    
    print(f"ğŸ“‚ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘ ì¤‘...")
    try:
        for ext in image_extensions:
            image_files.extend(input_path.rglob(f'*{ext}'))
            image_files.extend(input_path.rglob(f'*{ext.upper()}'))
    except Exception as e:
        print(f"âŒ íŒŒì¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0, 0, 0
    
    print(f"ğŸ“‚ ì´ {len(image_files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    
    if len(image_files) == 0:
        print(f"âŒ {input_path}ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0, 0
    
    # ì¤‘ë³µ ì œê±° (ì„ íƒì‚¬í•­)
    if remove_exact_duplicates_only:
        # íŒŒì¼ í•´ì‹œ ê¸°ë°˜ ì™„ì „ ì¤‘ë³µë§Œ ì œê±° (ê¶Œì¥)
        image_files, duplicates = remove_exact_duplicates(image_files)
    
    if remove_duplicates:
        # pHash ê¸°ë°˜ ìœ ì‚¬ ì¤‘ë³µ ì œê±° (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
        image_files, duplicates = deduplicate_cuts(image_files, hash_threshold)
        print(f"âœ… ìœ ì‚¬ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(duplicates)}ê°œ ì œê±°, {len(image_files)}ê°œ ìœ ì§€")
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    processed_count = 0
    filtered_count = 0
    error_count = 0
    metadata = {}
    error_log = []  # ì—ëŸ¬ ë¡œê·¸ ì¶”ê°€
    
    print("\nğŸ¨ ì „ì²˜ë¦¬ ì‹œì‘...")
    for img_path in tqdm(image_files, desc="Processing"):
        # ì¶œë ¥ ê²½ë¡œ ìƒì„± (ì›ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
        relative_path = img_path.relative_to(input_path)
        output_file = output_path / relative_path
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        result, style_features, status = preprocess_webtoon_cut(img_path, target_size)
        
        if result is not None:
            # í•œê¸€ ê²½ë¡œ ì €ì¥: cv2.imencode ì‚¬ìš©
            is_success, buffer = cv2.imencode('.jpg', result)
            if is_success:
                buffer.tofile(str(output_file))
                metadata[str(relative_path)] = {
                    'original_path': str(img_path),
                    'status': status,
                    'style_features': style_features
                }
                processed_count += 1
            else:
                error_count += 1
                error_log.append((str(img_path), "imencode failed"))
        elif "Excessive spacing" in status:
            filtered_count += 1
        else:
            error_count += 1
            error_log.append((str(img_path), status))
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_file = output_path / 'preprocessing_metadata.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump({
            'total_input': len(image_files),
            'processed': processed_count,
            'filtered': filtered_count,
            'errors': error_count,
            'target_size': target_size,
            'remove_duplicates': remove_duplicates,
            'files': metadata
        }, f, indent=2, ensure_ascii=False)
    
    # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
    if len(error_log) > 0:
        error_log_file = output_path / 'error_log.txt'
        with open(error_log_file, 'w', encoding='utf-8') as f:
            f.write(f"ì´ {len(error_log)}ê°œ ì—ëŸ¬\n\n")
            for path, error_msg in error_log[:100]:  # ì²˜ìŒ 100ê°œë§Œ
                f.write(f"{path}\n  -> {error_msg}\n\n")
        print(f"âš ï¸  ì—ëŸ¬ ë¡œê·¸: {error_log_file}")
    
    print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   - ì²˜ë¦¬ ì„±ê³µ: {processed_count}ê°œ")
    print(f"   - í•„í„°ë§ë¨: {filtered_count}ê°œ")
    print(f"   - ì˜¤ë¥˜: {error_count}ê°œ")
    print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_file}")
    
    return processed_count, filtered_count, error_count

# ==================== 6. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ====================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ì›¹íˆ° ì»· ì´ë¯¸ì§€ ì „ì²˜ë¦¬')
    parser.add_argument('--input_dir', type=str, default=r'D:\Crawling\Naver',
                        help='ì›ë³¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, default=r'D:\Crawling\Naver_Processed',
                        help='ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    parser.add_argument('--target_size', type=int, default=320,
                        help='ë¦¬ì‚¬ì´ì¦ˆ ëª©í‘œ í¬ê¸° (ì§§ì€ ë³€ ê¸°ì¤€, ê¸°ë³¸ê°’: 320)')
    parser.add_argument('--remove_duplicates', action='store_true',
                        help='pHash ê¸°ë°˜ ìœ ì‚¬ ì¤‘ë³µ ì œê±° í™œì„±í™” (ëŠë¦¼)')
    parser.add_argument('--hash_threshold', type=int, default=2,
                        help='pHash í•´ë° ê±°ë¦¬ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 2, ë‚®ì„ìˆ˜ë¡ ì—„ê²©)')
    parser.add_argument('--no_exact_duplicate_removal', action='store_true',
                        help='ì™„ì „ ì¤‘ë³µ ì œê±° ë¹„í™œì„±í™”')
    parser.add_argument('--create_zip', action='store_true',
                        help='ê²°ê³¼ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•')
    parser.add_argument('--visualize', action='store_true',
                        help='ì „ì²˜ë¦¬ ì „í›„ ë¹„êµ ì´ë¯¸ì§€ ìƒì„±')
    
    args = parser.parse_args()
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    print(f"\n{'='*60}")
    print(f"ğŸ¨ ì›¹íˆ° ì»· ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹œì‘")
    print(f"{'='*60}")
    print(f"ğŸ“‚ ì…ë ¥ ê²½ë¡œ: {args.input_dir}")
    print(f"ğŸ“‚ ì¶œë ¥ ê²½ë¡œ: {args.output_dir}")
    print(f"ğŸ¯ ëª©í‘œ í¬ê¸°: {args.target_size}px")
    print(f"ğŸ” ì™„ì „ ì¤‘ë³µ ì œê±°: {'ë¹„í™œì„±í™”' if args.no_exact_duplicate_removal else 'í™œì„±í™” (ê¶Œì¥)'}")
    print(f"ğŸ” ìœ ì‚¬ ì¤‘ë³µ ì œê±°: {'í™œì„±í™”' if args.remove_duplicates else 'ë¹„í™œì„±í™”'}")
    if args.remove_duplicates:
        print(f"   - pHash ì„ê³„ê°’: {args.hash_threshold}")
    print(f"{'='*60}\n")
    
    processed, filtered, errors = process_webtoon_directory(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        target_size=args.target_size,
        remove_duplicates=args.remove_duplicates,
        hash_threshold=args.hash_threshold,
        remove_exact_duplicates_only=not args.no_exact_duplicate_removal
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"âœ… ì²˜ë¦¬ ì„±ê³µ: {processed}ê°œ")
    print(f"ğŸš« í•„í„°ë§ë¨: {filtered}ê°œ (ê³¼ë„í•œ ì—¬ë°±)")
    print(f"âš ï¸  ì˜¤ë¥˜: {errors}ê°œ")
    print(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {args.output_dir}/preprocessing_metadata.json")
    print(f"{'='*60}\n")


# ==================== 7. ê°„ë‹¨ ì‹¤í–‰ ì˜ˆì‹œ ====================

"""
í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰:

# 1. ê¸°ë³¸ ì‹¤í–‰ (ì™„ì „ ì¤‘ë³µë§Œ ì œê±°, ê¶Œì¥)
python webtoon_preprocessing.py

# 2. ì™„ì „ ì¤‘ë³µ ì œê±°ë„ ë„ê¸° (ëª¨ë“  íŒŒì¼ ë³´ì¡´)
python webtoon_preprocessing.py --no_exact_duplicate_removal

# 3. ìœ ì‚¬ ì¤‘ë³µë„ ì œê±° (ëŠë¦¬ì§€ë§Œ ë” ë§ì´ ì œê±°)
python webtoon_preprocessing.py --remove_duplicates --hash_threshold 2

# 4. ZIP + ì‹œê°í™” ì¶”ê°€
python webtoon_preprocessing.py --create_zip --visualize

# 5. ë‹¤ë¥¸ ê²½ë¡œ ì§€ì •
python webtoon_preprocessing.py \
    --input_dir "D:\ë‹¤ë¥¸ê²½ë¡œ\ì›ë³¸" \
    --output_dir "D:\ë‹¤ë¥¸ê²½ë¡œ\ì²˜ë¦¬ì™„ë£Œ"
"""
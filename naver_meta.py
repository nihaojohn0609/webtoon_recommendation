import os
import json
import time
import random
from bs4 import BeautifulSoup

# Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ---------------------------
# ì„¤ì •
# ---------------------------
BASE_DIR = r"D:\Crawling\Naver_Processed"
OUTPUT_JSON = "webtoon_metadata.json"
FAILED_TITLES_JSON = "failed_titles.json"

# ê³µì‹ ì¥ë¥´ ê¸°ì¤€ (í•˜ë“œì½”ë”©)
OFFICIAL_GENRES = {
    "ì¼ìƒ", "ê°œê·¸", "íŒíƒ€ì§€", "ì•¡ì…˜", "ë“œë¼ë§ˆ", "ìˆœì •", "ê°ì„±", "ìŠ¤ë¦´ëŸ¬", 
    "ë¬´í˜‘/ì‚¬ê·¹", "ìŠ¤í¬ì¸ ", "ë¡œë§¨ìŠ¤", "í•™ì›", "ê³µí¬", "ë¯¸ìŠ¤í„°ë¦¬", "ì‹œëŒ€ê·¹",
    "BL", "GL", "ì˜´ë‹ˆë²„ìŠ¤", "ì—í”¼ì†Œë“œ", "ìŠ¤í† ë¦¬", "ë¡œíŒ", "ë¬´í˜‘", "ì‚¬ê·¹", "ì„±ì¸"
}

# ---------------------------
# ë¸Œë¼ìš°ì € ì„¤ì • í•¨ìˆ˜
# ---------------------------
def create_driver():
    chrome_options = Options()
    # headlessë¥¼ Trueë¡œ í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ì•ˆ ëœ¨ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë•ë‹ˆë‹¤. (ì†ë„ ë¹ ë¦„)
    # ì²˜ìŒì—” ì˜ ë˜ëŠ”ì§€ ë³´ë ¤ë©´ Falseë¡œ ë‘ì„¸ìš”.
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    
    # ë´‡ íƒì§€ íšŒí”¼ ì˜µì…˜
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# ---------------------------
# ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ (Selenium ì´ìš©)
# ---------------------------
def scrape_with_selenium(driver, title_id, folder_title):
    url = f"https://comic.naver.com/webtoon/list?titleId={title_id}"
    
    metadata = {
        "title": folder_title, # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë”ëª… ì‚¬ìš©
        "titleId": title_id,
        "writer": "",
        "genre": [],
        "keywords": [],
        "summary": "",
        "status": ""
    }

    try:
        # 1. ë¸Œë¼ìš°ì €ë¡œ ì´ë™
        driver.get(url)
        
        # 2. í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ì•ˆì „í•˜ê²Œ 2~3ì´ˆ ëŒ€ê¸°)
        # ë„¤ì´ë²„ ì›¹íˆ°ì€ ë™ì  ë¡œë”©ì´ë¼ ì‹œê°„ì´ ì¡°ê¸ˆ í•„ìš”í•©ë‹ˆë‹¤.
        time.sleep(random.uniform(2, 3))
        
        # 3. í˜„ì¬ ë¸Œë¼ìš°ì €ì— ë³´ì´ëŠ” HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")
        
        # -------------------------------------------------
        # ë°ì´í„° ì¶”ì¶œ (HTML êµ¬ì¡° ë¶„ì„)
        # -------------------------------------------------
        
        # [ì œëª©] Meta íƒœê·¸ê°€ ê°€ì¥ ì •í™•í•¨
        og_title = soup.select_one('meta[property="og:title"]')
        if og_title:
            metadata["title"] = og_title.get("content", "").strip()

        # [ìš”ì•½]
        og_desc = soup.select_one('meta[property="og:description"]')
        if og_desc:
            metadata["summary"] = og_desc.get("content", "").strip()

        # [ì‘ê°€] 'ContentMetaInfo__category' í´ë˜ìŠ¤ê°€ í¬í•¨ëœ íƒœê·¸ ì°¾ê¸°
        # (í´ë˜ìŠ¤ ì´ë¦„ì´ ì¼ë¶€ ë°”ë€Œì–´ë„ ì°¾ì„ ìˆ˜ ìˆê²Œ ë¶€ë¶„ ë§¤ì¹­ ì‚¬ìš©)
        writers = []
        author_tags = soup.find_all(class_=lambda c: c and 'ContentMetaInfo__category' in c)
        for tag in author_tags:
            text = tag.get_text(strip=True)
            # ê¸€, ê·¸ë¦¼, ì›ì‘ ë“±ì˜ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‘ê°€ ì´ë¦„ìœ¼ë¡œ ê°„ì£¼
            if any(x in text for x in ['ê¸€', 'ê·¸ë¦¼', 'ì›ì‘']):
                clean_name = text.replace('ê¸€', '').replace('ê·¸ë¦¼', '').replace('ì›ì‘', '').strip()
                if clean_name:
                    writers.append(clean_name)
        
        # ì¤‘ë³µ ì œê±° í›„ ì €ì¥
        metadata["writer"] = ', '.join(sorted(set(writers))) if writers else ""

        # [ìƒíƒœ] (ì—°ì¬ì¤‘/ì™„ê²° ë“±)
        status_tag = soup.find(class_=lambda c: c and 'ContentMetaInfo__info_item' in c)
        if status_tag:
            text = status_tag.get_text(strip=True)
            metadata["status"] = text.split('âˆ™')[0].strip() if 'âˆ™' in text else text

        # [íƒœê·¸ & ì¥ë¥´] ë²„íŠ¼ì´ë‚˜ ë§í¬ ì¤‘ #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒ ëª¨ë‘ ìˆ˜ì§‘
        # íŠ¹ì • div ì•ˆì„ ì°¾ì§€ ì•Šê³  ì „ì²´ì—ì„œ ì°¾ìŒ (êµ¬ì¡° ë³€ê²½ ë°©ì–´)
        all_buttons = soup.find_all(['a', 'button'])
        
        for tag in all_buttons:
            text = tag.get_text(strip=True)
            if text.startswith('#') and len(text) > 1:
                clean_tag = text.replace('#', '').strip()
                
                # ì¥ë¥´/í‚¤ì›Œë“œ ë¶„ë¥˜
                if clean_tag in OFFICIAL_GENRES:
                    if clean_tag not in metadata["genre"]:
                        metadata["genre"].append(clean_tag)
                else:
                    if clean_tag not in metadata["keywords"]:
                        metadata["keywords"].append(clean_tag)

        return metadata

    except Exception as e:
        print(f"[ERROR] Selenium ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ID={title_id}): {e}")
        return None

# ---------------------------
# ë©”ì¸ ì‹¤í–‰
# ---------------------------
def main():
    if not os.path.exists(BASE_DIR):
        print(f"[ERROR] í´ë” ê²½ë¡œ í™•ì¸ í•„ìš”: {BASE_DIR}")
        return

    # í´ë” ëª©ë¡ì—ì„œ ID ì¶”ì¶œ
    all_dirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]
    target_items = []
    for d in all_dirs:
        parts = d.split("_", 1)
        if len(parts) == 2 and parts[0].isdigit():
            target_items.append({"titleId": parts[0], "title": parts[1]})
    
    print(f"ì´ {len(target_items)}ê°œì˜ ì‘í’ˆì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...)")
    
    # ë¸Œë¼ìš°ì € ì‹œì‘
    driver = create_driver()
    
    metadata_list = []
    failed_list = []

    try:
        for idx, item in enumerate(target_items, 1):
            tid = item['titleId']
            folder_title = item['title']
            
            print(f"[{idx}/{len(target_items)}] '{folder_title}' ({tid}) ì ‘ì† ì¤‘...", end=" ")
            
            result = scrape_with_selenium(driver, tid, folder_title)
            
            if result:
                # ë°ì´í„° í™•ì¸
                has_genre = len(result['genre']) > 0
                has_keyword = len(result['keywords']) > 0
                
                if has_genre or has_keyword:
                    print(f"âœ… [ì„±ê³µ] ì¥ë¥´: {result['genre']} | í‚¤ì›Œë“œ: {len(result['keywords'])}ê°œ")
                else:
                    print(f"âš ï¸ [ì£¼ì˜] íƒœê·¸ ì—†ìŒ (í˜ì´ì§€ í™•ì¸ í•„ìš”)")
                
                metadata_list.append(result)
            else:
                print(f"âŒ [ì‹¤íŒ¨]")
                failed_list.append(item)
            
            # ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì•½ê°„ ëŒ€ê¸°
            # time.sleep(1) 

    except KeyboardInterrupt:
        print("\n[ì¤‘ë‹¨] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
        
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        driver.quit()
        
        # ì €ì¥
        with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
            json.dump(metadata_list, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ìµœì¢… ì €ì¥ ì™„ë£Œ: {OUTPUT_JSON}")
        
        if failed_list:
            with open(FAILED_TITLES_JSON, "w", encoding="utf-8") as f:
                json.dump(failed_list, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()